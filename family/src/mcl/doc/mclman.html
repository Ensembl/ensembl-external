
<html>
  <head>
    <style type="text/css">
      body {textalign: justify}
      body        {  color: #001111 ; background: white; }
      body        {  margin-left: 8%; margin-right: 8%; }
      a:link      { text-decoration: none; }
      a:active    { text-decoration: none; }
      a:visited   { text-decoration: none; }
    </style>
    <title> The mcl manual
    </title>
  </head>
<body>
<dl compact>
  <dt>1.<dd><a href="#name">NAME</a>
  <dt>2.<dd><a href="#synopsis">SYNOPSIS</a>
  <dt>3.<dd><a href="#description">DESCRIPTION</a>
  <dt>4.<dd><a href="#options">OPTIONS</a>
  <dt>5.<dd><a href="#files">FILES</a>
  <dt>6.<dd><a href="#environment">ENVIRONMENT</a>
  <dt>7.<dd><a href="#diagnostics">DIAGNOSTICS</a>
  <dt>8.<dd><a href="#bugs">BUGS</a>
  <dt>9.<dd><a href="#author">AUTHOR</a>
  <dt>10.<dd><a href="#history">HISTORY/CREDITS</a>
  <dt>11.<dd><a href="#seealso">SEE ALSO</a>
  <dt>12.<dd><a href="#references">REFERENCES</a>
  <dt>13.<dd><a href="#notes">NOTES</a>
</dl>

<a name="name"></a>
<h2>NAME</h2>

<p>
mcl - the Amsterdam implementation of the Markov Cluster Algorithm
(<b>MCL algorithm</b>)

<a name="synopsis"></a>
<h2>SYNOPSIS</h2>

<p>
<b>mcl</b> &lt;-|fname&gt;
<a href="#opt-I"><b>[-I</b> f (<i>inflation</i>)<b>]</b></a>
<a href="#opt-o"><b>[-o</b> str (<i>fname</i>)<b>]</b></a>

<p>
<b>mcl</b> &lt;-|fname&gt;
<a href="#opt-I"><b>[-I</b> f (<i>main inflation</i>)<b>]</b></a>
<a href="#opt-o"><b>[-o</b> str (<i>fname</i>)<b>]</b></a>
<a href="#opt-c"><b>[-c</b> f (<i>centering</i>)<b>]</b></a>
<a href="#opt-p"><b>[-p</b> f (<i>cutoff</i>)<b>]</b></a>
<a href="#opt-P"><b>[-P</b> n (<i>1/cutoff</i>)<b>]</b></a>
<a href="#opt-m"><b>[-m</b> n (<i>mark number</i>)<b>]</b></a>
<a href="#opt-M"><b>[-M</b> n (<i>mark number</i>)<b>]</b></a>
<a href="#opt-progress"><b>[-progress</b> n (<i>gauge</i>)<b>]</b></a>
<a href="#opt-i"><b>[-i</b> f (<i>initial inflation</i>)<b>]</b></a>
<a href="#opt-l"><b>[-l</b> n (<i>initial iteration number</i>)<b>]</b></a>
<a href="#opt-L"><b>[-L</b> n (<i>main iteration number</i>)<b>]</b></a>
<a href="#opt-v"><b>[-v</b> str (<i>verbosity type on</i>)<b>]</b></a>
<a href="#opt-V"><b>[-V</b> str (<i>verbosity type off</i>)<b>]</b></a>
<a href="#opt--verbose"><b>[--verbose</b> (<i>very</i>)<b>]</b></a>
<a href="#opt--silent"><b>[--silent</b> (<i>very</i>)<b>]</b></a>
<a href="#opt-nx"><b>[-nx</b> x (<i>track worst n</i>)<b>]</b></a>
<a href="#opt-ny"><b>[-ny</b> y (<i>track worst n</i>)<b>]</b></a>
<a href="#opt-a"><b>[-a</b> f (<i>loop weight</i>)<b>]</b></a>
<a href="#opt--recover"><b>[--recover</b> (<i>severe pruning</i>)<b>]</b></a>
<a href="#opt--select"><b>[--select</b> (<i>prune bound</i>)<b>]</b></a>
<a href="#opt-pct"><b>[-pct</b> f (<i>recover percentage</i>)<b>]</b></a>
<a href="#opt-t"><b>[-t</b> k (<i>#threads</i>)<b>]</b></a>
<a href="#opt-te"><b>[-te</b> k (<i>#expansion threads</i>)<b>]</b></a>
<a href="#opt-ti"><b>[-ti</b> k (<i>#inflation threads</i>)<b>]</b></a>
<a href="#opt--clone"><b>[--clone</b> (<i>when threading</i>)<b>]</b></a>
<a href="#opt-cloneat"><b>[-cloneat</b> n (<i>trigger</i>)<b>]</b></a>
<a href="#opt--rigid"><b>[--rigid</b> (<i>pruning</i>)<b>]</b></a>
<a href="#opt--adapt"><b>[--adapt</b> (<i>pruning</i>)<b>]</b></a>
<a href="#opt-ae"><b>[-ae</b> f (<i>adaptive pruning exponent</i>)<b>]</b></a>
<a href="#opt-af"><b>[-af</b> f (<i>adaptive pruning factor</i>)<b>]</b></a>
<a href="#opt--show"><b>[--show</b> (<i>print matrices to screen</i>)<b>]</b></a>
<a href="#opt-digits"><b>[-digits</b> n (<i>printing precision</i>)<b>]</b></a>
<a href="#opt--ascii"><b>[--ascii</b> (<i>output format</i>)<b>]</b></a>
<a href="#opt--binary"><b>[--binary</b> (<i>output format</i>)<b>]</b></a>
<a href="#opt--overlap"><b>[--overlap</b> (<i>keep it</i>)<b>]</b></a>
<a href="#opt--square"><b>[--square</b> (<i>input matrix</i>)<b>]</b></a>
<a href="#opt-preprune"><b>[-preprune</b> n (<i>input matrix</i>)<b>]</b></a>
<a href="#opt-dump"><b>[-dump</b> str (<i>type</i>)<b>]</b></a>
<a href="#opt-dumpi"><b>[-dumpi</b> i:j (<i>interval i..j-1</i>)<b>]</b></a>
<a href="#opt-dumpm"><b>[-dumpm</b> k (<i>dump i+0..i+k..</i>)<b>]</b></a>
<a href="#opt-dumpstem"><b>[-dumpstem</b> str (<i>file stem</i>)<b>]</b></a>
<a href="#opt-z"><b>[-z</b> (<i>show defaults</i>)<b>]</b></a>

<a name="description"></a>
<h2>DESCRIPTION</h2>

<p>
<b>mcl</b> implements the <b>MCL algorithm</b>, short for the <b>Markov cluster algorithm</b>,
a cluster algorithm for graphs developed by Stijn van Dongen at the Centre
for Mathematics and Computer Science in Amsterdam, the Netherlands. The
algorithm simulates flow using two simple algebraic operations on
matrices. The theory behind it is extensively described elsewhere (see
<a href="#references">REFERENCES</a>
). The program described here is a fast threaded
implementation written by the algorithm's creator with contributions
by several others.  Anton Enright co-implemented threading; see the
<a href="#history">HISTORY/CREDITS</a>
section for a complete account.  The implementation
is used for the TRIBES project in which large numbers of proteins are
clustered into families, and has become all the better from the
feedback this has generated.

<p>
The <a href="#opt-I"><b>-I</b>&nbsp;<i>f</i> option</a> is the main control,
affecting cluster granularity. Using <b>mcl</b> is as simple as
typing (assuming a file <i>proteins</i> contains a matrix/graph
in <b>mcl</b> input format)
<pre>
mcl proteins -I 2.0
</pre>

<p>
The above will result in a clustering written to the file
named <i>out.mcl</i>. The <b>mcl</b> input format is described in the
<b>mclformat(5)</b> entry. Clusterings are also stored as matrices -
this is again discussed in the <b>mclformat(5)</b> entry.
In finding good <b>mcl</b> parameter settings for a particular
domain, or in finding cluster structure at different levels of
granularity, one typically runs <b>mcl</b> multiple times for varying values
of f (refer to the <a href="#opt-I"><b>-I</b> entry</a> for
further information).

<p>
<b>mcl</b> expects a nonnegative matrix in the input file, or equivalently,
a weighted (possibly directed) graph.  <b>NOTE</b> - <b>mcl</b> interprets the
matrix entries or graph edge weights as <i>similarities</i>.
How your edge weights are computed may affect <b>mcl</b>'s performance.
In protein clustering, it is best to choose the negated
logarithm of the BLAST probabilities (see 
<a href="#references">REFERENCES</a>
).

<p>
For large graphs, you should learn about the three
groups of resource control parameters
<a href="#opt-P"><b>{</b><b>-P</b>/<b>-p</b><b>}</b></a>,
<a href="#opt-M"><b>{</b><b>-M</b>, <b>--recover</b>,
<b>--select</b>, <b>-pct</b><b>}</b></a>, and
<a href="#opt--adapt"><b>{</b><b>--adapt</b>,
<b>--rigid</b>, <b>-ae</b>, <b>-af</b><b>}</b></a>.
Whether <b>mcl</b> considers a graph large depends mainly on the graph
connectivity; a highly connected graph on 10,000 nodes is large to <b>mcl</b>
(so that you might want to tune resources) whereas a sparsely connected
graph on 100,000 nodes may be business as usual.
If graphs are really huge, the time to read a graph from file
can be shortened by converting the input graph from ascii mcl format
to binary mcl format with <b>mcxconvert(1)</b>.

<p>
<b>mcl</b>'s default parameters should make it quite fast in almost all
circumstances. For highly connected graphs, the defaults may result
in clusterings that can be improved by tuning resources.  If you
are lazy, learn about the <a href="#opt-P"><b>-P</b> option</a> only.
Increasing its value leads to more accurate computations, but slows
<b>mcl</b> down.  The parameters in the <a href="#opt-M"><b>-M</b> group</a>
and the <a href="#opt--adapt"><b>--adapt</b> group</a> may really help in
making <b>mcl</b> much and much faster again while retaining cluster quality.
<b>mcl</b> has been used to generate good protein clusters on 133k proteins,
taking 10 minutes running time on a Compaq ES40 system with four alpha
EV6.7 processors.  Two other groups of interest are the thread-related
options (you can specify the number of threads to use)
<a href="#opt-t"><b>{</b><b>-t</b>, <b>-te</b>, <b>-ti</b>, 
<b>--clone</b>, <b>-cloneat</b><b>}</b></a>
and the verbosity-related options
<a href="#opt--verbose"><b>{</b><b>--verbose</b>, <b>--silent</b>, <b>-v</b>, 
<b>-V</b><b>}</b></a>.
The default settings are shown with <b>-z</b>, and for graphs with
at most 12 nodes or so you can view the MCL matrix iterands on screen
by supplying <a href="#opt--show"><b>--show</b></a>
(this may give some more feeling).

<p>
The first option is the input file name (see the <b>mclformat(5)</b>
entry for its expected format), or a single hyphen to read from stdin.
The rationale is that you typically do several runs with different
parameters, and in command line mode it is pleasant if you do not have
to skip over an immutable parameter all the time.

<p>
In the 
<a href="#options">OPTIONS</a>
section options are listed in order of
importance, with related options grouped together.

<p>
The creator of this page feels that manual pages are a valuable resource,
that online html documentation is also a good thing to have, and
that info pages are way <i>way</i> ahead of their time. The
<a href="#notes">NOTES</a>
section explains how this page was created.

<a name="options"></a>
<h2>OPTIONS</h2>

<dl compact>
<dt style="margin-bottom:1em"><a name="opt-I"></a><b>-I</b> f (<i>inflation</i>)
<dd>
Sets the main inflation value to f. This value is the main handle
for affecting cluster granularity. It is usually chosen somewhere
in the range [1.2-5.0]. <b>-I</b>&nbsp;<b>5.0</b> will tend to result
in fine-grained clusterings, and <b>-I</b>&nbsp;<b>1.2</b> will tend to
result in very coarse grained clusterings. Your mileage will vary
depending on the characteristics of your data.  That is why it is
a good idea to test the quality and coherency of your clusterings
using <b>clmdist(1)</b> and <b>clminfo(1)</b>.
This will most likely reveal that
certain values of <b>-I</b> are simply not right for your data.

<p>
A second option for affecting cluster granularity is the
<a href="#opt-c"><b>-c</b> option</a>.
It may possibly increase granularity.

<p>
With low values for <b>-I</b>, like <b>-I</b>&nbsp;<b>1.2</b>, you should be
prepared to use more resources in order to maintain quality of
clusterings, i.e. increase the argument to the
<a href="#opt-P"><b>-P</b> option</a>.
<dt style="margin-bottom:1em"><a name="opt-o"></a><b>-o</b> str (<i>fname</i>)
<dd>
Output the clustering to file named fname. The default file name
is out.mcl. It is possible to send the clustering to stdout
by supplying <b>-o</b>&nbsp;<b>-</b>. The clustering is output in the
mcl matrix format; see the <b>mclformat(5)</b> entry for 
more information on this.
<dt style="margin-bottom:1em"><a name="opt-c"></a><b>-c</b> f (<i>centering</i>)
<dd>
The larger the value of f the more nodes are attached
to themselves rather than their neighbours, the more
expansion (the spreading of flow through the graph) is
opposed, and the more fine-grained clusterings tend to be.

<p>
f should be chosen greater than or equal to 1.0.
The default is f=1.0.

<p>
This option has a much weaker effect than the <b>-I</b>
option, but it can be useful depending on your data.
<dt style="margin-bottom:1em"><a name="opt-p"></a><b>-p</b> f (<i>cutoff</i>)
<dd>
See the <b>-P</b> entry immediately below.
<dt style="margin-bottom:1em"><a name="opt-P"></a><b>-P</b> n (<i>1/cutoff</i>)
<dd>
If you do not know nor want to know the mcl internals,
just ignore the additional information below
and follow the recommendations. They are quite straightforward.

<p>
The intent of pruning is that many small entries are
removed while retaining much of the stochastic mass
of the original vector. MCL iterands are theoretically
known to be sparse in a weighted sense, and this
manoever effectively perturbs the MCL process a little
in order to obtain matrices that are genuinely sparse,
thus keeping the computation tractable.
An example of monitoring pruning can be found
in the discussion of <b>-v</b>&nbsp;<b>pruning</b>
under the <a href="#opt--verbose"><b>--verbose</b> entry</a>.

<p>
After computing a new (column stochastic) matrix vector during
expansion (which is matrix multiplication c.q. squaring), all
entries that are smaller than <i>cutoff</i> are removed, implying
that no more than n entries can possibly survive. This is the
default pruning algorithm of <b>mcl</b>, and it is not smart but quite
effective.  After pruning, vectors are rescaled to be stochastic
again.  The cutoff can be supplied either by <b>-p</b>, or as
the inverse value by <b>-P</b>. The latter is more intuitive, if
your intuition is like mine.

<p>
This default pruning scheme is called
<i>rigid pruning</i>. The alternative is called <i>adaptive pruning</i>
- see the <a href="#opt--adapt"><b>--adapt</b> entry</a>.  The latter
scheme is somewhat more flexible, and both schemes can be made
smarter using the <a href="#opt-M"><b>-M</b> entry</a>.

<p>
For graphs with at most a few thousand nodes, the default value for
<b>-P</b> (equalling 1000) is almost certainly sufficient. For
larger graphs, it may still be sufficient, but if you have graphs
with tens or hundreds of thousand of nodes (or even more), you
should experiment with values up until somewhere below 10,000. If
you go above P=2000, you should definitely try the <b>--adapt</b>
switch.  You should probably also try using the <b>-M</b>&nbsp;<i>n</i>
flag, setting it to the same value as the P value or to somewhere
in the range P/2-P.

<p>
Judge the right value of n by the time <b>mcl</b> consumes and the quality
of clusters it produces.  Use the option <b>-v</b>&nbsp;<b>pruning</b>
to see how much mass is kept on average. The average is computed
over all matrix vectors, over the worst x instances, and the worst
y instances (x and y are tunable using <b>-nx</b> and <b>-ny</b>).
Try to obtain parameter settings for which the overall average is
always above 90% (mass kept), and for which the x and y averages
quickly rise above 90% (see the discussion of <b>-v</b>&nbsp;<b>pruning</b>
below the <a href="#opt--verbose"><b>--verbose</b></a> entry).

<p>
For large graphs, choose n larger. For highly connected graphs,
choose n larger. For small inflation values f, i.e. if you aim for
coarse-grained clusterings (see the <a href="#opt-I"><b>-I</b> entry</a>),
choose n larger.  The latter is because coarse grained-clusterings
induce MCL iterands/matrices that are less sparse, so that pruning
must be applied more carefully.
<dt style="margin-bottom:1em"><a name="opt-m"></a><b>-m</b> n (<i>mark number</i>)
<dd>
See the <b>-M</b>&nbsp;<i>n</i> entry below.
<dt style="margin-bottom:1em"><a name="opt-pct"></a><b>-pct</b> f (<i>recover percentage</i>)
<dd>
See the <b>-M</b>&nbsp;<i>n</i> entry below.
<dt style="margin-bottom:1em"><a name="opt--recover"></a><b>--recover</b> (<i>severe pruning</i>)
<dd>
See the <b>-M</b>&nbsp;<i>n</i> entry below.
<dt style="margin-bottom:1em"><a name="opt--select"></a><b>--select</b> (<i>prune bound</i>)
<dd>
See the <b>-M</b>&nbsp;<i>n</i> entry below.
<dt style="margin-bottom:1em"><a name="opt-M"></a><b>-M</b> n (<i>mark number</i>)
<dd>
The mark number n identifies an integer threshold that is used
for two purposes once the initial pruning of a vector
is completed. Suppose initial pruning yields a
vector with k entries.

<p>
<b>First</b>, if k is larger than m, <b>mcl</b> will prune the vector
further of its smallest entries until at most m entries result
(in case of ties slightly more than m may result).

<p>
<b>Second</b>, if k is smaller than m and the current fraction
of retained mass is smaller than a given threshold (default 95,
tunable using <b>-pct</b>), <b>mcl</b> will undo pruning and make sure
that at least m entries are kept.

<p>
Turning on the verbosity mode <b>-v</b>&nbsp;<b>pruning</b> shows you how
many selections and recoveries <b>mcl</b> had to perform during each
iteration.  It also shows you the mass averages of the pruned
vectors after the first selection stage, and the mass averages
of the vectors as finally pruned.  One more thing it shows: the
distribution of the vector <i>footprints</i> (i.e. the number of nonzero
entries) before and after pruning.

<p>
It is possible to select only one of the behaviours
described above by using <b>-m</b>&nbsp;<i>n</i> rather than
<b>-M</b>&nbsp;<i>n</i>, and specifying either <b>--recover</b>
or <b>--select</b>. So <b>-M</b>&nbsp;<i>n</i> is equivalent
with <b>-m</b>&nbsp;<i>n</i> and those two switches turned on.

<p>
Specifying just <b>-m</b>&nbsp;<i>n</i> will cause <b>mcl</b> to report on
the number of select and recovery actions it <i>should</i>
have done, provided you turn on the verbosity mode
<b>-v</b>&nbsp;<b>pruning</b>. See the discussion of the latter option
under the <a href="#opt--verbose"><b>--verbose</b> entry</a> for an
example of mcl reporting on pruning.
<dt style="margin-bottom:1em"><a name="opt--rigid"></a><b>--rigid</b> (<i>pruning</i>)
<dd>
See the <b>--adapt</b> entry below.
<dt style="margin-bottom:1em"><a name="opt-ae"></a><b>-ae</b> f (<i>adaptive pruning exponent</i>)
<dd>
See the <b>--adapt</b> entry below.
<dt style="margin-bottom:1em"><a name="opt-af"></a><b>-af</b> f (<i>adaptive pruning factor</i>)
<dd>
See the <b>--adapt</b> entry below.
<dt style="margin-bottom:1em"><a name="opt--adapt"></a><b>--adapt</b> (<i>pruning</i>)
<dd>
The default <b>mcl</b> pruning regime is extremely straightforward;
given a (stochastic) column vector computed during expansion (c.q.
matrix multiplication c.q. squaring), all its entries that are
bigger than a fixed threshold (tunable by <a href="#opt-P"><b>-p</b>
or <b>-P</b></a>) are removed, and the vector is rescaled to have
sum one.  This default behaviour is called <i>rigid pruning</i>.
It being the default renders the switch <b>--rigid</b>
currently useless.

<p>
The options discussed under the <a href="#opt-M"><b>-M</b>&nbsp;<i>n</i> entry</a>
already enable more versatile behaviour.
The options discussed here enable the computation
of a threshold that depends on the homogeneity characteristics
of a vector. This behaviour is triggered by supplying
<b>--adapt</b>.

<p>
It must be emphasized that i) the <b>--adapt</b> behaviour is
orthogonal to the functionality provided by <b>-M</b>&nbsp;<i>n</i> and
its friends and ii) is depending also on the value corresponding
with the <b>-P</b>&nbsp;<i>n</i> option.  When using <b>--adapt</b>, you
typically use the <b>-P</b>&nbsp;<i>n</i> and the <b>-M</b>&nbsp;<i>n</i> options
as well, and you can and should use higher values for both then
you would do without using <b>--adapt</b>.

<p>
All that said, <b>--adapt</b> triggers this behaviour: Given a
stochastic vector v, its mass center of order two is computed,
which is the sum of each entry squared. The mass center of v,
call it c, is strongly related to its homogeneity properties
(see 
<a href="#references">REFERENCES</a>
).  The threshold T is computed as 1/f *
pow(c, e), where e and f are the arguments to the <b>-af</b>&nbsp;<i>f</i>
and <b>-ae</b>&nbsp;<i>e</i> options respectively (both default to 4.0).
For either e or f decreasing it means that T becomes larger.
<i>Finally, T is maxed with the rigid threshold value</i>, which
can be altered using either <b>-p</b>&nbsp;<i>f</i> or <b>-P</b>&nbsp;<i>n</i>.
The latter is why you should increase the <b>-P</b> parameter n
(so that the rigid threshold is decreased) once you switch to
adaptive pruning. The adaptive threshold should be the main factor
controlling pruning, with the rigid threshold acting as a safeguard
that does not take over too often.

<p>
This may seem complicated, but the rules are actually quite simple,
and you may just disregard the definition of T.
Combining these options with recovery and/or selection via
the mark number (the <b>-M</b>&nbsp;<i>n</i> option) yields many
ways of tuning the <b>mcl</b> parameters to the graph type
found in a particular domain. The main benefit is that
they provide a way of drastically reducing computation
time while retaining clustering quality.
Example settings:
<pre>
mcl fname -I somevalue --adapt -P 1000 -M 1000
mcl fname -I somevalue --adapt -P 1200 -M 1000
mcl fname -I somevalue --adapt -P 2000 -M 1500
mcl fname -I somevalue --adapt -P 3000 -M 2000
</pre>

<p>
Try to obtain parameter settings for which the overall average
of kept mass (after completion of pruning, before rescaling) is
always above 90%, and for which the x and y averages
quickly rise above 90% (see the discussion of <b>-v</b>&nbsp;<b>pruning</b>
under the <a href="#opt--verbose"><b>--verbose</b></a> entry).
<dt style="margin-bottom:1em"><a name="opt-nx"></a><b>-nx</b> x (<i>track worst n</i>)
<dd>
See in the <b>--verbosity</b> entry below the discussion
of the <i>pruning</i> mode.
<dt style="margin-bottom:1em"><a name="opt-ny"></a><b>-ny</b> y (<i>track worst n</i>)
<dd>
See in the <b>--verbosity</b> entry below the discussion
of the <i>pruning</i> mode.
<dt style="margin-bottom:1em"><a name="opt-v"></a><b>-v</b> str (<i>verbosity type on</i>)
<dd>
See the <b>--verbosity</b> entry below.
<dt style="margin-bottom:1em"><a name="opt-V"></a><b>-V</b> str (<i>verbosity type off</i>)
<dd>
See the <b>--verbosity</b> entry below.
<dt style="margin-bottom:1em"><a name="opt--silent"></a><b>--silent</b> (<i>very</i>)
<dd>
See the <b>--verbosity</b> entry below.
<dt style="margin-bottom:1em"><a name="opt--verbose"></a><b>--verbose</b> (<i>very</i>)
<dd>
These are the different verbosity modes:

<p>
<b>mcl</b><br>
<b>progress</b><br>
<b>pruning</b><br>
<b>all</b>

<p>
where <i>all</i> means all three previous modes.
<b>--verbose</b> and <b>-v</b>&nbsp;<b>all</b>
turn them all on, <b>--silent</b> and <b>-V</b>&nbsp;<b>all</b>
turn them all off. <b>-v</b>&nbsp;<i>str</i> and <b>-V</b>&nbsp;<i>str</i>
turn on/off the single mode <i>str</i> (for <i>str</i>
equal to one of <b>mcl</b>, <b>progress</b>, or <b>pruning</b>).
Each verbosity mode is given its own entry below.
<dt style="margin-bottom:1em"><b>-v</b>&nbsp;<b>mcl</b>
<dd>
All this currently does is telling, when all computations are
finished, how  much clusters the resulting clustering contains.
It previously used to map each MCL iterand onto an  overlapping
clustering,  and  output some  statistics of  that clustering. (MCL
theory shows  that MCL iterands posses structural properties
generalizing those of  the final limit). This behaviour may be
revived in the future.
<dt style="margin-bottom:1em"><b>-v</b>&nbsp;<b>progress</b>
<dd>
This mode causes <b>mcl</b> to emit an ascii gauge
for each single matrix multiplication. It uses some
default length for the gauge, which can be altered by
the <b>-progress</b>&nbsp;<i>k</i> option. Simply using the latter
will also turn on this verbosity mode.
This mode can give you quickly an idea how long an <b>mcl</b>
run might take. If you use threading
(see the <a href="#opt-t"><b>-t</b> entry</a> and its friends),
this option may slow down the program a little (relative to
<b>-V</b>&nbsp;<b>progress</b>, not relative to a single-CPU <b>mcl</b> run).
<dt style="margin-bottom:1em"><b>-v</b>&nbsp;<b>pruning</b>
<dd>
Pruning mode causes <b>mcl</b> to emit several statistics
related to the pruning process.

<p align=justify>
<b>Selection and recovery</b>
<br>
The number of selections and recoveries <b>mcl</b> had to perform
during each iteration. See the <a href="#opt-M"><b>-M</b> entry</a>.
It also shows the number of vectors for which the mass
after final pruning was below the fraction defined by
the <a href="#opt-pct"><b>-pct</b> option</a> as a percentage
(default probably 95).

<p align=justify>
<b>Initial and final vector size distributions</b>
<br>
The distribution of the vector footprints (i.e. the number of nonzero
entries) before and after pruning. This is assembled in a terse
(horrid if you will) ascii output format, looking as follows
(with some context stripped, noting that the data for three
multiplications is shown):
<pre>
----------------------------------------------------
&nbsp;mass percentages  | distribution of vec footprints|
&nbsp;        |         |__ compose ________ prune _____|
&nbsp; prune  | final   |000  00   0    |000  00   0    |
all ny nx|all ny nx|8532c8532c8532c|8532c8532c8532c|
---------.---------.---------------.---------------.
&nbsp;98 88 86  99 95 95 ____0224567899@ ______02346899@
&nbsp;97 88 85  99 95 95 __113456789@@@@ ______02346899@
&nbsp;98 89 87  99 95 95 __123455689@@@@ ______02346789@
&nbsp;...
</pre>

<p align=justify>
This particular output was generated (and truncated) from clustering
a protein graph on 9000 nodes with settings <b>-I</b>&nbsp;<b>1.4</b>,
<b>-P</b>&nbsp;<b>2000</b>, and <b>-M</b>&nbsp;<b>1200</b>.

<p align=justify>
The header entries 8532c85.. with the zeroes on top indicate
thresholds going from 8000, 5000, 2000, 1250, 800, all the way down
to 30, 20, and 12. The character 'c' signifies the base 12.5 (for
no apparent reason). The
second entry '2' (after '0') signifies that roughly 20 percent
of all the vectors had size (#nonzero entries) between 800 and
1250. Likewise, 80 percent had size between 500 and 800. The '0'
entries signify a fraction somewhere below 5 percent, and the '@'
entries signify a fraction somewhere above 95 percent.

<p align=justify>
Two columns are listed, one for the composed vector footprints
(i.e. after squaring), and the other for the final vector
footprints after pruning.

<p align=justify>
<b>Initial and final vector mass distributions</b>
<br>
The mass averages of the pruned vectors after the first
selection stage are shown, and the mass averages of the vectors
as finally pruned. For both cases, three averages are shown:
the average of all vectors, the average of the worst x cases,
and the average of the worst y cases. The values x and y
default to something like x=10 and y=100; check the
<a href="#opt-z"><b>-z</b> option</a> to be sure. They can be
changed using <b>-nx</b>&nbsp;<b>x</b> and <b>-ny</b>&nbsp;<b>y</b>.

<p align=justify>
In the example above it is clearly seen that
many entries could be removed while retaining much
of the stochastic mass. The effect of the <b>-M</b>
parameter is also clear: the final averages are higher
than the initial averages, as a result of  <b>mcl</b> undoing
some overenthousiastic pruning.
<dt style="margin-bottom:1em"><a name="opt-progress"></a><b>-progress</b> k (<i>gauge</i>)
<dd>
If k&gt;0 then for each matrix multiplication <b>mcl</b> will print an
ascii gauge telling how far it is. The gauge will be (in some
cases approximately) k characters long. If k&lt;0 then <b>mcl</b> will
emit a gauge that is extended by one character after every |k|
vectors computed. For large graphs, this option has been known
to ease the pain of impatience. If k=0 then <b>mcl</b> will print a
message only after every matrix multiplication, and not during
matrix multiplication. This can be useful when you want <b>mcl</b> to be
as speedy as possible, for example when using parallellized mode
(as monitoring progress requires thread communication).
For parallellization (by threading) see the
<a href="#opt-t"><b>-t</b> entry</a>.
<dt style="margin-bottom:1em"><a name="opt-te"></a><b>-te</b> k (<i>#expansion threads</i>)
<dd>
See the <b>-t</b>&nbsp;<i>k</i> entry below.
<dt style="margin-bottom:1em"><a name="opt-ti"></a><b>-ti</b> k (<i>#inflation threads</i>)
<dd>
See the <b>-t</b>&nbsp;<i>k</i> entry below.
<dt style="margin-bottom:1em"><a name="opt--clone"></a><b>--clone</b> (<i>when threading</i>)
<dd>
See the <b>-t</b>&nbsp;<i>k</i> entry below.
<dt style="margin-bottom:1em"><a name="opt-cloneat"></a><b>-cloneat</b> n (<i>trigger</i>)
<dd>
See the <b>-t</b>&nbsp;<i>k</i> entry below.
<dt style="margin-bottom:1em"><a name="opt-t"></a><b>-t</b> k (<i>#threads</i>)
<dd>
The <b>-t</b> options are self-explanatory.  Note that threading
inflation is hardly useful, as inflation is orders of magnitude
faster than expansion.  Also note that threading is only useful
if you have a multi-processor system.

<p>
The <b>--clone</b>
option says to give each thread its own copy of the matrix being
expanded/squared.  The latter option can be further controlled
using the <b>--cloneat</b>&nbsp;<i>k</i> option. Copies are only made if
the source matrix (the one to be squared) has on average at least
k positive entries per vector.

<p>
When threading, it is best not to turn on pruning verbosity
mode if you are letting mcl run unattended, unless you want to
scrutinize its output later. This is because it makes <b>mcl</b> run
somewhat slower, although the difference is not dramatic.
<dt style="margin-bottom:1em"><a name="opt-l"></a><b>-l</b> n (<i>initial iteration number</i>) (small letter ell)
<dd>
The number of times <b>mcl</b> will use a different inflation value
before it switches to the (main) inflation given by the <b>-I</b>
(capital eye) option. The different value is called <i>initial
inflation</i> and is tunable using the <a href="#opt-i"><b>-i</b>&nbsp;<i>f</i>
option</a> (default value f=2.0).  The default value (to <b>-l</b>)
is zero.  This option supplies new ways of affecting cluster
granularity, e.g. by supplying
<pre>
mcl proteins -i 1.4 -l 2 -I 4.0
</pre>

<p>
one lets expansion prevail during the first two iterations,
followed by inflation catching up (in a figurative way of writing).
This may be useful in certain cases, but this type of experiment
is certainly secondary to simply varying <b>-I</b> (capital eye).
<dt style="margin-bottom:1em"><a name="opt-L"></a><b>-L</b> n (<i>main iteration number</i>)
<dd>
Normally, <b>mcl</b> computes the MCL process until it has converged
fully to a doubly idempotent matrix. The number of iterations
required is typically somewhere in the range 10-100.
The first few iterations generally take the longest time.
The <b>-L</b> option can be used to specify the number of
iterations <b>mcl</b> may do at most. When this number is reached,
<b>mcl</b> will output the clustering associated with the iterand
last computed.
<dt style="margin-bottom:1em"><a name="opt-i"></a><b>-i</b> f (<i>initial inflation</i>)
<dd>
The inflation value used during the first n iterations,
where n is specified by the <b>-l</b> (ell) option.
By default, n=0 and f=2.0.
<dt style="margin-bottom:1em"><a name="opt-a"></a><b>-a</b> f (<i>loop weight</i>)
<dd>
<b>Deprecated</b>&nbsp;&nbsp;Like the <a href="#opt-c"><b>-c</b> option</a>,
except that it adds loops of absolute weight.  This can be intuitive
when testing with simple graphs, however, using <b>-c</b>&nbsp;<b>f</b>
will in this case have exactly the same effect as <b>-a</b>&nbsp;<b>f</b>,
so do use the former.
<dt style="margin-bottom:1em"><a name="opt-dumpi"></a><b>-dumpi</b> i:j (<i>interval i..j-1</i>)
<dd>
Dump during iterations i..j-1. See the <b>-dump</b>&nbsp;<i>str</i> entry below.
<dt style="margin-bottom:1em"><a name="opt-dumpm"></a><b>-dumpm</b> k (<i>dump i+0..i+k..</i>)
<dd>
Sampling rate: select only these iterations in the dump interval.
See the <b>-dump</b>&nbsp;<i>str</i> entry below.
<dt style="margin-bottom:1em"><a name="opt-dumpstem"></a><b>-dumpstem</b> stem (<i>file stem</i>)
<dd>
Set the the stem for file names of dumped
objects (default <i>mcl</i>). See the <b>-dump</b>&nbsp;<i>str</i> entry below.
<dt style="margin-bottom:1em"><a name="opt-dump"></a><b>-dump</b> str (<i>type</i>)
<dd>
<i>str</i> can be of the following types.

<p>
<b>att</b><br>
<b>ite</b><br>
<b>cls</b>

<p>
Repeated use is allowed. The <b>att</b> option says to output a vector
measuring for each node how much it is attracted to itself (which
measures the extent to which nodes are situated in the core of a
cluster).  It is somewhat forlorn because the other mcl utilities
(see the 
<a href="#seealso">SEE ALSO</a>
section) can not yet utilize its output.
The <b>ite</b> option writes <b>mcl</b> iterands to file.  The <b>cls</b>
option writes clusterings associated with <b>mcl</b> iterands to file.

<p>
The <b>-dumpstem</b> sets the stem for file names of dumped
objects (default <i>mcl</i>).  The <b>-dumpi</b> and <b>-dumpm</b>
allow a selection of iterands to be made.
<dt style="margin-bottom:1em"><a name="opt-digits"></a><b>-digits</b> n (<i>printing precision</i>)
<dd>
See the <a href="#opt--show"><b>--show</b> entry</a> below.
<dt style="margin-bottom:1em"><a name="opt--show"></a><b>--show</b> (<i>print matrices to screen</i>)
<dd>
Print matrices to screen. The number of significant digits to be
printed can be tuned with <b>-digits</b>&nbsp;<i>n</i>. An 80-column screen
allows graphs (matrices) of size up to 12(x12) to be printed with
three digits precision (behind the comma), and of size up to 14(x14)
with two digits. This can give you an idea of how <b>mcl</b> operates,
and what the effect of pruning is. Use e.g. <b>-M</b>&nbsp;<b>6</b> for such
a small graph and view the MCL matrix iterands with <b>--show</b>.
<dt style="margin-bottom:1em"><a name="opt--ascii"></a><b>--ascii</b> (<i>output format</i>)
<dd>
See the <a href="#opt--binary"><b>--binary</b> entry</a> below.
<dt style="margin-bottom:1em"><a name="opt--binary"></a><b>--binary</b> (<i>output format</i>)
<dd>
Write the resulting clustering in binary mcl format rather
than ascii mcl format (the default). Note that <b>mcxconvert(1)</b>
can be used to convert each one into the other.
See the <b>mclformat(5)</b> entry for more information.
<dt style="margin-bottom:1em"><a name="opt--overlap"></a><b>--overlap</b> (<i>keep it</i>)
<dd>
Keep overlap. In theory, <b>mcl</b> may generate a clustering that
contains overlap, although this almost never happens in practice,
as it requires some particular type of symmetry to be present in
the input graph (not just any symmetry will do).  Mathematically
speaking, this is a conjecture and not a theorem, but I am willing
to eat my shoe if it does not hold (for marzipan values
of shoe).  It is easy though to construct an input graph for which
certain <b>mcl</b> settings result in overlap - for example a line graph
on an odd number of nodes.  The default is to remove overlap should
it occur.

<p>
This option has more than theoretical use because <b>mcl</b> is able
to generate clusterings associated with intermediate iterands.
For these clusterings, overlap is more than a theoretical
possibility, and will often occur. If you specify
the <a href="#opt-L"><b>-L</b>&nbsp;<i>k</i></a> option, <b>mcl</b> will output the
clustering associated with the last iterand computed, and
it may well contain overlap.

<p>
This option has no effect on the clusterings that are
output when using <a href="#opt-dump"><b>-dump</b>&nbsp;<i>cls</i></a> -
the default for those is that overlap is not touched,
and this default can not yet be overridden.
<dt style="margin-bottom:1em"><a name="opt--square"></a><b>--square</b> (<i>input matrix</i>)
<dd>
Square the input matrix, and proceed with using the square as
input for the MCL algorithm.  This option was introduced for
easily clustering the two domains of a bipartite graphs.  It seems
preferable though to have a more customizable way of preparing the
separate graphs for the two domains, so this makes the option look
a bit forlorn.  Perhaps it has other uses too.
<dt style="margin-bottom:1em"><a name="opt-preprune"></a><b>-preprune</b> n (<i>input matrix</i>)
<dd>
For each column vector (node) in the input matrix (graph) <b>mcl</b> will 
keep the n entries (outgoing edges) of that vector (node) that
have largest weight and remove the rest.
<dt style="margin-bottom:1em"><a name="opt-z"></a><b>-z</b> (<i>show defaults</i>)
<dd>
Show defaults for tunable parameters.
<dt style="margin-bottom:1em"><b>That's All Folks</b>
<dd>
for now.
</dl><!-- ^_^ -->

<a name="files"></a>
<h2>FILES</h2>

<p>
There are currently no resource nor configuration files.
The mcl matrix format is described in the <b>mclformat(5)</b> entry.

<a name="environment"></a>
<h2>ENVIRONMENT</h2>

<p>
Currently, no environmental issues with <b>mcl</b>.

<a name="diagnostics"></a>
<h2>DIAGNOSTICS</h2>

<p>
If <b>mcl</b> issues a diagnostic error, it will most likely be
because the input matrix could not be parsed succesfully.
<b>mcl</b> tries to be helpful in describing the kind of parse error.
The mcl matrix format is described in the
<b>mclformat(5)</b> entry.

<a name="bugs"></a>
<h2>BUGS</h2>

<p>
No known bugs at this time. Please send bug reports to stijn@diff.nl

<a name="author"></a>
<h2>AUTHOR</h2>

<p>
Stijn van Dongen.

<a name="history"></a>
<h2>HISTORY/CREDITS</h2>

<p>
The MCL algorithm was conceived in spring 1996 by the present author.
The first implementation of the MCL algorithm followed that spring
and summer. It was written in Perl and proved the viability of
the algorithm. The implementation described here began its life in
autumn 1997. The first versions of the vital matrix library
were designed jointly by Stijn van Dongen and Annius Groenink in
the period Oktober 1997 - May 1999. The efficient matrix-vector
multiplication routine was written by Annius. This routine is
without significant changes still one of the cornerstones of this
MCL implementation.

<p>
Since May 1999 all MCL libraries have seen much development and
redesign by the present author. Matrix-matrix multiplication has been
rewritten several times to take full advantage of the sparseness
properties of the stochastic matrices brought forth by the MCL
algorithm. This mostly concerns the issue of pruning - removal of
small elements in a stochastic column in order to keep  matrices
sparse.

<p>
Very instructive was  that around April 2001 Rob  Koopman pointed out
that selecting  the k largest  elements out of  a collection of  n is
best done  using a  min-heap. This  was the key  to the  second major
rewrite (now counting three) of the MCL pruning schemes, resulting in
much faster code, generally producing  a more accurate computation of
the MCL process.

<p>
In May 2001  Anton Enright initiated the parallellization  of the
<b>mcl</b> code and threaded inflation. From this example, Stijn threaded
expansion.  This was great, as the MCL data structures  and operands
(normal matrix multiplication and Hadamard multiplication) just beg
for parallellization.

<p>
In October 2001 Joost van Baal packaged <b>mcl</b> for Debian GNU/Linux
and contributed thus much to the organization of the sources.

<a name="seealso"></a>
<h2>SEE ALSO</h2>

<p>
<b>mclformat(5)</b> - a description of the mcl matrix format.

<p>
Several utilities come with <b>mcl</b>, facilitating analysis
and comparison of different clusterings.

<p>
<b>clmdist(1)</b> - compute the split/join distance between two 
clusterings.

<p>
<b>clminfo(1)</b> - computes a performance measure saying how well
a clustering captures the edge weights of the input graph. Useful
for comparing different clusterings on the same graph, best used in
conjunction with <b>clmdist(1)</b> - because comparing clusterings at
different levels of granularity should somewhat change the performance
interpretation. The latter issue is discussed in the <b>clmdist(1)</b>
entry.

<p>
<b>clmmeet(1)</b> - compute the intersection of a set clusterings,
i.e. the largest clustering that is a subclustering of all. Useful
for measuring the consistency of a set of different clusterings
at supposedly different levels of granularity (in conjunction with
<b>clmdist(1)</b>).

<p>
<b>clmproject(1)</b> - compute how well nodes fit into the cluster
in which they are located (for a given clustering) by looking at
the (weighted) percentage of its edges going to that same cluster.
Computes also the cohesiveness of a cluster, by computing and averaging
the above over all nodes in a cluster.  Useful for inspecting local
cluster structure.

<p>
<b>mcxmorph(1)</b> - compute products of matrices, possibly
intermixed with  pruning, normalization, transposing, and making a
matrix characteristic.

<p>
<b>mcxsubs(1)</b> - compute a submatrix of a given matrix, where row
and column index sets can be specified as lists of indices combined
with list of clusters in a given clustering. Useful for inspecting
local cluster structure.

<p>
<b>mcxconvert(1)</b> - convert matrices from ascii mcl format to
binary mcl format or vice versa.

<a name="references"></a>
<h2>REFERENCES</h2>

<dl compact>
<dt style="margin-bottom:1em"><b>Graph Clustering by Flow Simulation</b> (thesis)
<dd>
<a href="http://www.library.uu.nl/digiarchief/dip/diss/1895620/inhoud.htm">http://www.library.uu.nl/digiarchief/dip/diss/1895620/inhoud.htm</a>
<dt style="margin-bottom:1em"><b>A cluster algorithm for graphs</b> (technical report)
<dd>
<a href="http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z">http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z</a>
<dt style="margin-bottom:1em"><b>A stochastic uncoupling process for graphs</b> (technical report)
<dd>
<a href="http://www.cwi.nl/ftp/CWIreports/INS/INS-R0011.ps.Z">http://www.cwi.nl/ftp/CWIreports/INS/INS-R0011.ps.Z</a>
<dt style="margin-bottom:1em"><b>Performance criteria for graph clustering and Markov cluster experiments</b> (technical report)
<dd>
<a href="http://www.cwi.nl/ftp/CWIreports/INS/INS-R0012.ps.Z">http://www.cwi.nl/ftp/CWIreports/INS/INS-R0012.ps.Z</a>
<dt style="margin-bottom:1em"><b>TRIBE-MCL - Automatic Protein Family detection using Markov Clustering</b> (preprint)
<dd>
Not yet available.
</dl><!-- ^_^ -->

<a name="notes"></a>
<h2>NOTES</h2>
This page was generated from <b>YAM</b> macros.  Both html and nroff pages can be
created from the same source without having to bother with all the usual
conversion problems, while keeping some level of sophistication in the
typesetting. The YAM primitives only provide macro expansion and filter
capabilities; the proof of the typesetting is in striking the macros right.
YAM syntax resembles TeX syntax.

</html>
</body>
